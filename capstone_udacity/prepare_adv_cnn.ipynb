{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "\n",
    "def concatenate_features(inps):\n",
    "    \"\"\"\n",
    "    An input is under shape (nb_samples, nb_features)\n",
    "    :param inps: a list of inputs used for multi-input model\n",
    "    :return: the concatenated features under shape (nb_samples, nb_feature1 + nb_feature2 + ...)\n",
    "    \"\"\"\n",
    "    if not inps:\n",
    "        return []\n",
    "    nb = len(inps[0])\n",
    "    for inp in inps:\n",
    "        assert(len(inp) == nb)\n",
    "    concatenated = []\n",
    "    for i in range(nb):\n",
    "        row = []\n",
    "        for inp in inps:\n",
    "            row.extend(inp[i])\n",
    "        concatenated.append(row)\n",
    "    return concatenated\n",
    "\n",
    "class SeqVectorizer(TfidfVectorizer):\n",
    "    def transform2seq(self, raw_docs):\n",
    "        \"\"\"\n",
    "        transform docs composed of words or n-grams into a sequence index \n",
    "        \"\"\"\n",
    "        analyzer = self.build_analyzer()\n",
    "        index_docs = []\n",
    "        for doc in raw_docs:\n",
    "            index_docs.append([])\n",
    "            for feature in analyzer(doc):\n",
    "                index = self.vocabulary_.get(feature, -1)\n",
    "                if index > 0:\n",
    "                    index_docs[-1].append(index)\n",
    "        return index_docs\n",
    "\n",
    "\n",
    "def pad_sequence(seq, repl, maxlen):\n",
    "    \"\"\"\n",
    "    Cut the part of sequence exceding maxlen, Or fill the sequence with \"repl\" until maxlen\n",
    "    \"\"\"\n",
    "    out_seq = []\n",
    "    nb = 0\n",
    "    for t in seq:\n",
    "        if nb < maxlen:\n",
    "            out_seq.append(t)\n",
    "            nb += 1\n",
    "        else:\n",
    "            return out_seq\n",
    "    while len(out_seq) < maxlen:\n",
    "        out_seq.append(repl)\n",
    "    return out_seq\n",
    "\n",
    "svs = []\n",
    "\n",
    "w_v = SeqVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000) #keep 20000 most frequent words\n",
    "svs.append((w_v, 250)) #keep the first 250 words for a comment\n",
    "\n",
    "c_v3 = SeqVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 3),\n",
    "    max_features=20000) #keep 20000 most frequent 3*-grams\n",
    "svs.append((c_v3, 400)) #keep the first 400 3-grams for a comment\n",
    "\n",
    "for sv, _ in svs:\n",
    "    sv.fit(train_text)\n",
    "\n",
    "print(\"vocabulery lengths:\")\n",
    "for sv, _ in svs:\n",
    "    print(len(sv.vocabulary_))\n",
    "\n",
    "X_train = concatenate_features([\n",
    "    pad_sequences(sv.transform2seq(train_text), maxlen=maxl,\n",
    "                  value=len(sv.vocabulary_), truncating=\"post\", padding=\"post\") for sv, maxl in svs\n",
    "])\n",
    "\n",
    "X_test = concatenate_features([\n",
    "    pad_sequences(sv.transform2seq(test_text), maxlen=maxl,\n",
    "                  value=len(sv.vocabulary_), truncating=\"post\", padding=\"post\") for sv, maxl in svs\n",
    "])\n",
    "\n",
    "np.save(\"temp_res/train_mulinp250400\", X_train)\n",
    "np.save(\"temp_res/test_mulinp250400\", X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
